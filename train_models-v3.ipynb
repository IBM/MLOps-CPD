{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "## Training Notebook "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### The following cell is a way to get the utility script required for this notebook. \nSince IBM CPD SaaS doesn't have a filesystem, this is the only reliable way to get scripts on the cloud environment.\n\n```\n!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git\n```\n\n\u26a0\ufe0f Run the following cells only if you are executing on IBM CPD SaaS."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!mv MLOps-CPD MLOps_CPD"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: ibm_watson_machine_learning in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (1.0.283)\nRequirement already satisfied: ibm_watson_studio_pipelines in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (0.2.12)\nCollecting ibm_aigov_facts_client\n  Downloading ibm_aigov_facts_client-1.0.48-py3-none-any.whl (148 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m148.8/148.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (4.8.2)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (0.8.9)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (2.26.0)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (2022.12.7)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (0.3.3)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (1.26.7)\nRequirement already satisfied: ibm-cos-sdk==2.11.* in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (2.11.0)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (21.3)\nRequirement already satisfied: pandas<1.5.0,>=0.24.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_machine_learning) (1.3.4)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cos-sdk==2.11.*->ibm_watson_machine_learning) (0.10.0)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cos-sdk==2.11.*->ibm_watson_machine_learning) (2.11.0)\nRequirement already satisfied: ibm-cos-sdk-core==2.11.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cos-sdk==2.11.*->ibm_watson_machine_learning) (2.11.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk==2.11.*->ibm_watson_machine_learning) (2.8.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_studio_pipelines) (4.1.1)\nRequirement already satisfied: ibm-cloud-sdk-core>=3.11.3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_studio_pipelines) (3.16.2)\nRequirement already satisfied: attrs>=21.2.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_studio_pipelines) (21.2.0)\nRequirement already satisfied: responses>=0.13.4 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_studio_pipelines) (0.22.0)\nRequirement already satisfied: pytest>=6.2.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_watson_studio_pipelines) (6.2.5)\nCollecting docutils<0.16,>=0.10\n  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m547.6/547.6 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_aigov_facts_client) (1.20.3)\nCollecting mlflow-skinny==1.28.0\n  Downloading mlflow_skinny-1.28.0-py3-none-any.whl (3.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting python-magic\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: pillow in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_aigov_facts_client) (9.0.1)\nRequirement already satisfied: sqlparse>=0.3.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm_aigov_facts_client) (0.4.3)\nCollecting timeout-decorator\n  Downloading timeout-decorator-0.5.0.tar.gz (4.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting databricks-cli<1,>=0.8.7\n  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm_aigov_facts_client) (3.19.6)\nRequirement already satisfied: entrypoints<1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm_aigov_facts_client) (0.3)\nCollecting gitpython<4,>=2.1.0\n  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm_aigov_facts_client) (5.4.1)\nRequirement already satisfied: cloudpickle<3 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm_aigov_facts_client) (2.2.1)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm_aigov_facts_client) (8.0.4)\nRequirement already satisfied: pytz<2023 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from mlflow-skinny==1.28.0->ibm_aigov_facts_client) (2021.3)\nRequirement already satisfied: PyJWT<3.0.0,>=2.4.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from ibm-cloud-sdk-core>=3.11.3->ibm_watson_studio_pipelines) (2.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from importlib-metadata->ibm_watson_machine_learning) (3.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from packaging->ibm_watson_machine_learning) (3.0.4)\nRequirement already satisfied: iniconfig in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pytest>=6.2.5->ibm_watson_studio_pipelines) (1.1.1)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pytest>=6.2.5->ibm_watson_studio_pipelines) (1.0.0)\nRequirement already satisfied: py>=1.8.2 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pytest>=6.2.5->ibm_watson_studio_pipelines) (1.10.0)\nRequirement already satisfied: toml in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from pytest>=6.2.5->ibm_watson_studio_pipelines) (0.10.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests->ibm_watson_machine_learning) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from requests->ibm_watson_machine_learning) (2.0.4)\nRequirement already satisfied: types-toml in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from responses>=0.13.4->ibm_watson_studio_pipelines) (0.10.8.5)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from lomond->ibm_watson_machine_learning) (1.15.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /opt/conda/envs/Python-3.9/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny==1.28.0->ibm_aigov_facts_client) (3.2.1)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: timeout-decorator, databricks-cli\n  Building wheel for timeout-decorator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for timeout-decorator: filename=timeout_decorator-0.5.0-py3-none-any.whl size=5028 sha256=9833b63189bb0b98a5d7d5a6b965c351e5e0274f28e7466f5b9ea5d1502b8ce1\n  Stored in directory: /home/wsuser/.cache/pip/wheels/34/d1/39/bc7d2752126d645738da07c5a85ecdbfa50e1e3a5a6fd39f75\n  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142894 sha256=29c8ed745152f90ba3278faae82654c19ae3ed5d5f01fe383fae743811a93fbb\n  Stored in directory: /home/wsuser/.cache/pip/wheels/69/79/af/5dedd3bac0031e64e081bd37505d5d55d9cd566fbad743d259\nSuccessfully built timeout-decorator databricks-cli\nInstalling collected packages: timeout-decorator, smmap, python-magic, docutils, gitdb, databricks-cli, gitpython, mlflow-skinny, ibm_aigov_facts_client\nSuccessfully installed databricks-cli-0.17.4 docutils-0.15.2 gitdb-4.0.10 gitpython-3.1.31 ibm_aigov_facts_client-1.0.48 mlflow-skinny-1.28.0 python-magic-0.4.27 smmap-5.0.0 timeout-decorator-0.5.0\n"
                }
            ],
            "source": "!pip install ibm_watson_machine_learning ibm_watson_studio_pipelines ibm_aigov_facts_client"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'ibm_aigov_facts_client'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/wsuser/ipykernel_332/143745636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mibm_aigov_facts_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIGovFactsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mibm_watson_studio_pipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWSPipelines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mibm_watson_machine_learning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAPIClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ibm_aigov_facts_client'"
                    ]
                }
            ],
            "source": "from botocore.client import Config\nfrom ibm_botocore.client import Config\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split,cross_val_score, KFold\nfrom sklearn.metrics import roc_auc_score,confusion_matrix,plot_confusion_matrix,plot_roc_curve,f1_score,auc,roc_curve,accuracy_score\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom ibm_aigov_facts_client import AIGovFactsClient\nfrom ibm_watson_studio_pipelines import WSPipelines\nfrom ibm_watson_machine_learning import APIClient\nimport warnings\nimport os, types\nimport pandas as pd\nimport numpy as np\nimport ibm_boto3\nimport seaborn as sns\nimport json\nimport pickle\nimport matplotlib.pyplot as plt\n\nfrom os.path import exists\nif exists(\"utils/fs_utils.py\") and exists(\"utils/catalog_utils.py\"):\n    from utils import fs_utils,catalog_utils\nelse:\n    # If utils/fs_utils.py and utils/catalog_utils.py exist we assume that you are running on CPD SaaS\n    # and will therefore import scripts from the freshly cloned repository\n    from MLOps_CPD.utils import fs_utils, catalog_utils\n\nwarnings.filterwarnings(\"ignore\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Succeeding cell contains the credentials for MLOps COS\n```\n## PROJECT COS \nAUTH_ENDPOINT = \"https://iam.cloud.ibm.com/oidc/token\"\nENDPOINT_URL = \"https://s3.private.us.cloud-object-storage.appdomain.cloud\"\nAPI_KEY_COS = \"xxx\"\nBUCKET_PROJECT_COS = \"mlops-donotdelete-pr-qxxcecxi1dtw94\"\n\n##MLOPS COS\nENDPOINT_URL_MLOPS = \"https://s3.jp-tok.cloud-object-storage.appdomain.cloud\"\nAPI_KEY_MLOPS = \"xxx\"\nCRN_MLOPS = \"xxx\"\nBUCKET_MLOPS  = \"mlops-asset\"\n\n##CATALOG\nCATALOG_NAME = \"MLOps-ns\"\n```"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## Retrieve cos credentials from global pipeline parameters\nimport json\n# Get json from environment and convert to string\nproject_cos_credentials = json.loads(os.getenv('project_cos_credentials'))\nmlops_cos_credentials = json.loads(os.getenv('mlops_cos_credentials'))\n\n## PROJECT COS \nAUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\nENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\nAPI_KEY_COS = project_cos_credentials['API_KEY']\nBUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n\n## MLOPS COS\nENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\nAPI_KEY_MLOPS = mlops_cos_credentials['API_KEY']\nCRN_MLOPS = mlops_cos_credentials['CRN']\nBUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Pipeline Params"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "CLOUD_API_KEY = os.getenv(\"cloud_api_key\")\n\nfilename = os.getenv(\"feature_pickle\")\n\nMODEL_NAME =os.getenv(\"model_name\")\nDEPLOYMENT_NAME =os.getenv(\"deployment_name\")\nCATALOG_NAME=\"mlops-ns\"\n\nproject_id = os.environ['PROJECT_ID']\nspace_id = os.getenv(\"model_name\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Utility Functions"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def download_file_cos(local_file_name,key):\n    cos = ibm_boto3.client(service_name='s3',\n                           ibm_api_key_id=API_KEY_MLOPS,\n                           ibm_service_instance_id=CRN_MLOPS,\n                           ibm_auth_endpoint=AUTH_ENDPOINT,\n                           config=Config(signature_version='oauth'),\n                           endpoint_url=ENDPOINT_URL_MLOPS)\n    try:\n        res=cos.download_file(Bucket=BUCKET_MLOPS,Key=key,Filename=local_file_name)\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print('File Downloaded')\n        \ndef read_data_from_mlops_cos(key):\n    def __iter__(self): return 0\n    MLOPS_DATA_STORE_client = ibm_boto3.client(\n        service_name='s3',\n        ibm_api_key_id=API_KEY_MLOPS,\n        ibm_service_instance_id=CRN_MLOPS,\n        ibm_auth_endpoint=AUTH_ENDPOINT,\n        config=Config(signature_version='oauth'),\n        endpoint_url=ENDPOINT_URL_MLOPS)\n\n    body = MLOPS_DATA_STORE_client.get_object(Bucket=BUCKET_MLOPS, Key=key)['Body']\n    # add missing __iter__ method, so pandas accepts body as file-like object\n    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n    gcf_df = pd.read_csv(body)\n    return gcf_df\n\ndef load_model(key, filename):\n    download_file_cos(key,filename)\n    with open (filename,\"rb\") as f:\n        pipeline = pickle.load(f)\n    return pipeline\n\ndef check_if_file_exists(filename):\n    mlops_client = ibm_boto3.client(\n        service_name='s3',\n        ibm_api_key_id=API_KEY_MLOPS,\n        ibm_service_instance_id=CRN_MLOPS,\n        ibm_auth_endpoint=AUTH_ENDPOINT,\n        config=Config(signature_version='oauth'),\n        endpoint_url=ENDPOINT_URL_MLOPS)\n    \n    for key in mlops_client.list_objects(Bucket=BUCKET_MLOPS)['Contents']:\n        files = key['Key']\n        if files == filename:\n            return True\n    return False\n\ndef save_data_in_cos(dictionary,filename,key):\n    \"\"\"\n\n    Save Data in IBM Cloud Object Storage\n\n    \"\"\"\n    try:\n        with open(filename+'.pkl', 'wb') as f:\n            pickle.dump(dictionary, f)\n        mlops_res = ibm_boto3.resource(\n            service_name='s3',\n            ibm_api_key_id=API_KEY_MLOPS,\n            ibm_service_instance_id=CRN_MLOPS,\n            ibm_auth_endpoint=AUTH_ENDPOINT,\n            config=Config(signature_version='oauth'),\n            endpoint_url=ENDPOINT_URL_MLOPS)\n\n        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename+'.pkl',key+'.pkl')\n        print(f\"File {filename} uploaded successfully\")\n    except Exception as e:\n        print(e)\n        print(\"File upload for {filename} failed\")\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Load the Saved Transformer from IBM COS "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pipeline = load_model(filename, filename)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Instantiate FactSheets Client"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "facts_client = AIGovFactsClient(api_key=CLOUD_API_KEY, experiment_name=\"CreditRiskModel\", container_type=\"project\", container_id=project_id, set_as_current_experiment=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Load Train Data and Test Data "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "train_data = read_data_from_mlops_cos('train_tfr.csv')\ntest_data = read_data_from_mlops_cos('test_tfr.csv')\ntrain_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Load train and test set"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "y_train = train_data['Risk']\n\ny_test = test_data['Risk']\n\n\nX_train = train_data.drop('Risk',axis=1)\n\nX_test = test_data.drop('Risk',axis=1)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Make validation set"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.model_selection import train_test_split\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Instantiate a Classifier "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel_lgb = XGBClassifier(learning_rate=0.1,max_depth=5,random_state=42)\n#model_lgb = LGBMClassifier(learning_rate=0.1,max_depth=5,random_state=42)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Append the Pipeline"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "estimator_step = ['model_lgb',model_lgb]\npipeline.steps.append(estimator_step)\n# if len(pipeline) >2:\n#     pipeline= list(pipeline[0]).append(pipeline[-1])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "pipeline.steps[0][1].fit(X_tr)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Baseline Model "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# model_pipeline = pipeline.fit(X_train,y_train)\nmodel_pipeline = pipeline.fit(X_tr,y_tr,model_lgb__verbose=5, model_lgb__eval_set=[(pipeline.steps[0][1].transform(X_val), y_val),(pipeline.steps[0][1].transform(X_tr), y_tr)])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "from lightgbm import plot_metric\nplot_metric(pipeline.steps[1][1])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Log the Train and Val loss"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "val_loss = pipeline.steps[1][1].evals_result_['valid_0']\ntrain_loss = pipeline.steps[1][1].evals_result_['valid_1']"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Save train and val loss to COS"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "save_data_in_cos(val_loss,'val_loss','val_loss')\nsave_data_in_cos(train_loss,'train_loss','train_loss')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Check if the files are copied in COS\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "files_copied_in_cos = check_if_file_exists('val_loss.pkl') and check_if_file_exists('val_loss.pkl')\nfiles_copied_in_cos"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Baseline Results of the Model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "predictions = pipeline.predict(X_test)\n\nprint(roc_auc_score(y_test,predictions))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "cv = KFold(n_splits=5, shuffle=True, random_state=1)\nscores = np.mean(cross_val_score(pipeline,X_train,y_train, cv=cv, n_jobs=-1,scoring='roc_auc'))\nprint(f\"The Cross Validated AUC_ROC Score is {scores}\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(pipeline.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(pipeline.score(X_test, y_test)))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_cm = confusion_matrix(y_test,predictions)\n\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in df_cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in df_cm.flatten()/np.sum(df_cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(df_cm, annot=labels, fmt='', cmap='Blues')\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\n\nprint(\"\\n\")\nprint (\"Area Under Curve: %.2f\" %auc(fpr, tpr))\nprint(\"\\n\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Store Model in the project"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "with open(\"model_pipeline.pkl\",'wb') as f:\n    pickle.dump(model_pipeline,f)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "WML_CREDENTIALS = {\n                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n                   \"apikey\": CLOUD_API_KEY\n            }"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "wml_client = APIClient(WML_CREDENTIALS)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Save and Log Models in AI Factsheets."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def save_log_facts():\n    ctutils = catalog_utils.CatalogUtils(\n        service_url=\"https://api.dataplatform.cloud.ibm.com\",\n        api_key=CLOUD_API_KEY,\n        project_id=project_id,\n        auth_url=\"https://iam.cloud.ibm.com/identity/token\")\n    catalog_id = ctutils.get_catalog_id_map()[CATALOG_NAME]\n    fsutils = fs_utils.FSUtils(wml_client=wml_client,catalog_id=catalog_id,project_id=project_id,bss_account_id='27ff418fedd6aedffb8dc6ae4164a1d2',space_id=space_id,facts_client=facts_client)\n    train_ref  = fsutils.prepare_training_reference(apikey=CLOUD_API_KEY,crn=CRN_MLOPS,bucket_name=BUCKET_MLOPS,endpoint=ENDPOINT_URL_MLOPS,training_file_name=\"german_credit_risk.csv\")\n    model_id = fsutils.save_model(model=model_pipeline,model_name=MODEL_NAME,model_entry_name=\"MLOPs\",model_entry_description=\"MLOps Model Entry\",target=\"Risk\",X=X_train,y=y_train,train_data_ref=train_ref)\n    \n    nb_name = \"train_models\"\n    nb_asset_id = \"b8d38cab-e373-4303-bd09-12e1086c9132\"\n    CPD_URL =\"https://dataplatform.cloud.ibm.com\"\n\n    nb_asset_url = \"https://\" + CPD_URL + \"/analytics/notebooks/v2/\" + nb_asset_id + \"?projectid=\" + project_id + \"&context=cpdaas\"\n\n    latestRunId = facts_client.runs.list_runs_by_experiment('1').sort_values('start_time').iloc[-1]['run_id']\n    facts_client.runs.set_tags(latestRunId, {\"Notebook name\": nb_name, \"Notebook id\": nb_asset_id, \"Notebook URL\" : nb_asset_url})\n    facts_client.export_facts.export_payload(latestRunId)\n\n    RUN_ID=facts_client.runs.get_current_run_id()\n    facts_client.export_facts.export_payload(RUN_ID)\n    \n    return model_id\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_id = save_log_facts()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Save Params in WS Pipeline"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "train_params = {}\ntrain_params['auc_roc'] = float(auc(fpr, tpr))\ntrain_params['training_done'] = True\ntrain_params['model_name'] = MODEL_NAME\ntrain_params['deployment_name'] = DEPLOYMENT_NAME\ntrain_params['model_id'] = model_id\ntrain_params['project_id'] = project_id\n# train_params['model_pipeline'] = \"/home/wsuser/work/model_pipeline.pkl\"\n\npipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\npipelines_client.store_results(train_params)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "vscode": {
            "interpreter": {
                "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}