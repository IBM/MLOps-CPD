{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "## Training Notebook "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The following cell is a way to get the utility script required for this notebook. \n",
                "Since IBM CPD SaaS doesn't have a filesystem, this is the only reliable way to get scripts on the cloud environment.\n",
                "\n",
                "```\n",
                "!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git\n",
                "```\n",
                "\n",
                "⚠️ Run the following cells only if you are executing on IBM CPD SaaS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!mv MLOps-CPD MLOps_CPD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from botocore.client import Config\n",
                "from ibm_botocore.client import Config\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
                "from sklearn.metrics import roc_auc_score,confusion_matrix,plot_confusion_matrix,plot_roc_curve,f1_score,auc,roc_curve,accuracy_score\n",
                "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
                "from ibm_aigov_facts_client import AIGovFactsClient\n",
                "from ibm_watson_studio_pipelines import WSPipelines\n",
                "from ibm_watson_machine_learning import APIClient\n",
                "import warnings\n",
                "import os, types\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import ibm_boto3\n",
                "import seaborn as sns\n",
                "import json\n",
                "import pickle\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from os.path import exists\n",
                "if exists(\"utils/fs_utils.py\") and exists(\"utils/catalog_utils.py\"):\n",
                "    from utils import fs_utils,catalog_utils\n",
                "else:\n",
                "    # If utils/fs_utils.py and utils/catalog_utils.py exist we assume that you are running on CPD SaaS\n",
                "    # and will therefore import scripts from the freshly cloned repository\n",
                "    from MLOps_CPD.utils import fs_utils, catalog_utils\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Succeeding cell contains the credentials for MLOps COS\n",
                "```\n",
                "## PROJECT COS \n",
                "AUTH_ENDPOINT = \"https://iam.cloud.ibm.com/oidc/token\"\n",
                "ENDPOINT_URL = \"https://s3.private.us.cloud-object-storage.appdomain.cloud\"\n",
                "API_KEY_COS = \"xxx\"\n",
                "BUCKET_PROJECT_COS = \"mlops-donotdelete-pr-qxxcecxi1dtw94\"\n",
                "\n",
                "##MLOPS COS\n",
                "ENDPOINT_URL_MLOPS = \"https://s3.jp-tok.cloud-object-storage.appdomain.cloud\"\n",
                "API_KEY_MLOPS = \"xxx\"\n",
                "CRN_MLOPS = \"xxx\"\n",
                "BUCKET_MLOPS  = \"mlops-asset\"\n",
                "\n",
                "##CATALOG\n",
                "CATALOG_NAME = \"MLOps-ns\"\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Retrieve cos credentials from global pipeline parameters\n",
                "import json\n",
                "# Get json from environment and convert to string\n",
                "project_cos_credentials = json.loads(os.getenv('project_cos_credentials'))\n",
                "mlops_cos_credentials = json.loads(os.getenv('mlops_cos_credentials'))\n",
                "\n",
                "## PROJECT COS \n",
                "AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n",
                "ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n",
                "API_KEY_COS = project_cos_credentials['API_KEY']\n",
                "BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n",
                "\n",
                "## MLOPS COS\n",
                "ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n",
                "API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n",
                "CRN_MLOPS = mlops_cos_credentials['CRN']\n",
                "BUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pipeline Params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filename = os.getenv(\"feature_pickle\")\n",
                "CLOUD_API_KEY = os.getenv(\"cloud_api_key\")\n",
                "\n",
                "# Model parameters\n",
                "MODEL_NAME =os.getenv(\"model_name\")\n",
                "DEPLOYMENT_NAME =os.getenv(\"deployment_name\")\n",
                "\n",
                "# Watson Studio parameters\n",
                "project_id = os.environ['PROJECT_ID']\n",
                "space_id = os.getenv(\"model_name\")\n",
                "CATALOG_NAME = os.getenv(\"catalog_name\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_file_cos(local_file_name,key):\n",
                "    cos = ibm_boto3.client(service_name='s3',\n",
                "                           ibm_api_key_id=API_KEY_MLOPS,\n",
                "                           ibm_service_instance_id=CRN_MLOPS,\n",
                "                           ibm_auth_endpoint=AUTH_ENDPOINT,\n",
                "                           config=Config(signature_version='oauth'),\n",
                "                           endpoint_url=ENDPOINT_URL_MLOPS)\n",
                "    try:\n",
                "        res=cos.download_file(Bucket=BUCKET_MLOPS,Key=key,Filename=local_file_name)\n",
                "    except Exception as e:\n",
                "        print(Exception, e)\n",
                "    else:\n",
                "        print('File Downloaded')\n",
                "        \n",
                "def read_data_from_mlops_cos(key):\n",
                "    def __iter__(self): return 0\n",
                "    MLOPS_DATA_STORE_client = ibm_boto3.client(\n",
                "        service_name='s3',\n",
                "        ibm_api_key_id=API_KEY_MLOPS,\n",
                "        ibm_service_instance_id=CRN_MLOPS,\n",
                "        ibm_auth_endpoint=AUTH_ENDPOINT,\n",
                "        config=Config(signature_version='oauth'),\n",
                "        endpoint_url=ENDPOINT_URL_MLOPS)\n",
                "\n",
                "    body = MLOPS_DATA_STORE_client.get_object(Bucket=BUCKET_MLOPS, Key=key)['Body']\n",
                "    # add missing __iter__ method, so pandas accepts body as file-like object\n",
                "    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
                "\n",
                "    gcf_df = pd.read_csv(body)\n",
                "    return gcf_df\n",
                "\n",
                "def load_model(key, filename):\n",
                "    download_file_cos(key,filename)\n",
                "    with open (filename,\"rb\") as f:\n",
                "        pipeline = pickle.load(f)\n",
                "    return pipeline\n",
                "\n",
                "def check_if_file_exists(filename):\n",
                "    mlops_client = ibm_boto3.client(\n",
                "        service_name='s3',\n",
                "        ibm_api_key_id=API_KEY_MLOPS,\n",
                "        ibm_service_instance_id=CRN_MLOPS,\n",
                "        ibm_auth_endpoint=AUTH_ENDPOINT,\n",
                "        config=Config(signature_version='oauth'),\n",
                "        endpoint_url=ENDPOINT_URL_MLOPS)\n",
                "    \n",
                "    for key in mlops_client.list_objects(Bucket=BUCKET_MLOPS)['Contents']:\n",
                "        files = key['Key']\n",
                "        if files == filename:\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def save_data_in_cos(dictionary,filename,key):\n",
                "    \"\"\"\n",
                "\n",
                "    Save Data in IBM Cloud Object Storage\n",
                "\n",
                "    \"\"\"\n",
                "    try:\n",
                "        with open(filename+'.pkl', 'wb') as f:\n",
                "            pickle.dump(dictionary, f)\n",
                "        mlops_res = ibm_boto3.resource(\n",
                "            service_name='s3',\n",
                "            ibm_api_key_id=API_KEY_MLOPS,\n",
                "            ibm_service_instance_id=CRN_MLOPS,\n",
                "            ibm_auth_endpoint=AUTH_ENDPOINT,\n",
                "            config=Config(signature_version='oauth'),\n",
                "            endpoint_url=ENDPOINT_URL_MLOPS)\n",
                "\n",
                "        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename+'.pkl',key+'.pkl')\n",
                "        print(f\"File {filename} uploaded successfully\")\n",
                "    except Exception as e:\n",
                "        print(e)\n",
                "        print(\"File upload for {filename} failed\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The code was removed by Watson Studio for sharing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the Saved Transformer from IBM COS "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline = load_model(filename, filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Instantiate FactSheets Client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "facts_client = AIGovFactsClient(api_key=CLOUD_API_KEY, experiment_name=\"CreditRiskModel\", container_type=\"project\", container_id=project_id, set_as_current_experiment=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Train Data and Test Data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data = read_data_from_mlops_cos('train_tfr.csv')\n",
                "test_data = read_data_from_mlops_cos('test_tfr.csv')\n",
                "train_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load train and test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train = train_data['Risk']\n",
                "\n",
                "y_test = test_data['Risk']\n",
                "\n",
                "\n",
                "X_train = train_data.drop('Risk',axis=1)\n",
                "\n",
                "X_test = test_data.drop('Risk',axis=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Make validation set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Instantiate a Classifier "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lightgbm import LGBMClassifier\n",
                "from xgboost import XGBClassifier\n",
                "\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "\n",
                "model_lgb = LGBMClassifier(learning_rate=0.09,max_depth=5,random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Append the Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "estimator_step = ['model_lgb',model_lgb]\n",
                "pipeline.steps.append(estimator_step)\n",
                "# if len(pipeline) >2:\n",
                "#     pipeline= list(pipeline[0]).append(pipeline[-1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "pipeline.steps[0][1].fit(X_tr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Baseline Model "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model_pipeline = pipeline.fit(X_train,y_train)\n",
                "model_pipeline = pipeline.fit(X_tr,y_tr,model_lgb__verbose=5, model_lgb__eval_set=[(pipeline.steps[0][1].transform(X_val), y_val),(pipeline.steps[0][1].transform(X_tr), y_tr)])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "from lightgbm import plot_metric\n",
                "plot_metric(pipeline.steps[1][1])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Log the Train and Val loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "val_loss = pipeline.steps[1][1].evals_result_['valid_0']\n",
                "train_loss = pipeline.steps[1][1].evals_result_['valid_1']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save train and val loss to COS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "save_data_in_cos(val_loss,'val_loss','val_loss')\n",
                "save_data_in_cos(train_loss,'train_loss','train_loss')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check if the files are copied in COS\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "files_copied_in_cos = check_if_file_exists('val_loss.pkl') and check_if_file_exists('val_loss.pkl')\n",
                "files_copied_in_cos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Baseline Results of the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictions = pipeline.predict(X_test)\n",
                "\n",
                "print(roc_auc_score(y_test,predictions))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
                "scores = np.mean(cross_val_score(pipeline,X_train,y_train, cv=cv, n_jobs=-1,scoring='roc_auc'))\n",
                "print(f\"The Cross Validated AUC_ROC Score is {scores}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# print the scores on training and test set\n",
                "\n",
                "print('Training set score: {:.4f}'.format(pipeline.score(X_train, y_train)))\n",
                "\n",
                "print('Test set score: {:.4f}'.format(pipeline.score(X_test, y_test)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_cm = confusion_matrix(y_test,predictions)\n",
                "\n",
                "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
                "group_counts = [\"{0:0.0f}\".format(value) for value in df_cm.flatten()]\n",
                "group_percentages = [\"{0:.2%}\".format(value) for value in df_cm.flatten()/np.sum(df_cm)]\n",
                "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in zip(group_names,group_counts,group_percentages)]\n",
                "labels = np.asarray(labels).reshape(2,2)\n",
                "sns.heatmap(df_cm, annot=labels, fmt='', cmap='Blues')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
                "\n",
                "fig, ax = plt.subplots()\n",
                "ax.plot(fpr, tpr)\n",
                "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.0])\n",
                "plt.rcParams['font.size'] = 12\n",
                "\n",
                "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
                "plt.ylabel('True Positive Rate (Sensitivity)')\n",
                "plt.grid(True)\n",
                "\n",
                "print(\"\\n\")\n",
                "print (\"Area Under Curve: %.2f\" %auc(fpr, tpr))\n",
                "print(\"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Store Model in the project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"model_pipeline.pkl\",'wb') as f:\n",
                "    pickle.dump(model_pipeline,f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "WML_CREDENTIALS = {\n",
                "                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
                "                   \"apikey\": CLOUD_API_KEY\n",
                "            }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wml_client = APIClient(WML_CREDENTIALS)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save and Log Models in AI Factsheets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_log_facts():\n",
                "    ctutils = catalog_utils.CatalogUtils(\n",
                "        service_url=\"https://api.dataplatform.cloud.ibm.com\",\n",
                "        api_key=CLOUD_API_KEY,\n",
                "        project_id=project_id,\n",
                "        auth_url=\"https://iam.cloud.ibm.com/identity/token\")\n",
                "    catalog_id = ctutils.get_catalog_id_map()[CATALOG_NAME]\n",
                "    fsutils = fs_utils.FSUtils(wml_client=wml_client,catalog_id=catalog_id,project_id=project_id,bss_account_id='27ff418fedd6aedffb8dc6ae4164a1d2',space_id=space_id,facts_client=facts_client)\n",
                "    train_ref  = fsutils.prepare_training_reference(apikey=CLOUD_API_KEY,crn=CRN_MLOPS,bucket_name=BUCKET_MLOPS,endpoint=ENDPOINT_URL_MLOPS,training_file_name=\"german_credit_risk.csv\")\n",
                "    model_id = fsutils.save_model(model=model_pipeline,model_name=MODEL_NAME,model_entry_name=\"MLOPs\",model_entry_description=\"MLOps Model Entry\",target=\"Risk\",X=X_train,y=y_train,train_data_ref=train_ref)\n",
                "    \n",
                "    nb_name = \"train_models\"\n",
                "    nb_asset_id = \"b8d38cab-e373-4303-bd09-12e1086c9132\"\n",
                "    CPD_URL =\"https://dataplatform.cloud.ibm.com\"\n",
                "\n",
                "    nb_asset_url = \"https://\" + CPD_URL + \"/analytics/notebooks/v2/\" + nb_asset_id + \"?projectid=\" + project_id + \"&context=cpdaas\"\n",
                "\n",
                "    latestRunId = facts_client.runs.list_runs_by_experiment('1').sort_values('start_time').iloc[-1]['run_id']\n",
                "    facts_client.runs.set_tags(latestRunId, {\"Notebook name\": nb_name, \"Notebook id\": nb_asset_id, \"Notebook URL\" : nb_asset_url})\n",
                "    facts_client.export_facts.export_payload(latestRunId)\n",
                "\n",
                "    RUN_ID=facts_client.runs.get_current_run_id()\n",
                "    facts_client.export_facts.export_payload(RUN_ID)\n",
                "    \n",
                "    return model_id\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id = save_log_facts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Params in WS Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_params = {}\n",
                "train_params['auc_roc'] = float(auc(fpr, tpr))\n",
                "train_params['training_done'] = True\n",
                "train_params['model_name'] = MODEL_NAME\n",
                "train_params['deployment_name'] = DEPLOYMENT_NAME\n",
                "train_params['model_id'] = model_id\n",
                "train_params['project_id'] = project_id\n",
                "# train_params['model_pipeline'] = \"/home/wsuser/work/model_pipeline.pkl\"\n",
                "\n",
                "pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n",
                "pipelines_client.store_results(train_params)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "vscode": {
            "interpreter": {
                "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
