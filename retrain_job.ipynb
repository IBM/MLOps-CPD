{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import os\nfrom datetime import datetime, timedelta\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\nimport time\n\nfrom MLOps_CPD.jenkins_jobs.pr_jenkins_job import JobRunner\n\nimport ibm_watson_openscale\nfrom ibm_cloud_sdk_core.authenticators import BearerTokenAuthenticator, IAMAuthenticator\nfrom ibm_watson_machine_learning import APIClient as wmlapiclient\nfrom ibm_watson_openscale import *\nfrom ibm_watson_openscale.supporting_classes import *\nfrom ibm_watson_openscale.supporting_classes.enums import *\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "CLOUD_API_KEY = os.getenv('CLOUD_API_KEY')\nSPACE_ID = os.getenv(\"SPACE_ID\")\nSERVICE_PROVIDER_ID = os.getenv(\"SERVICE_PROVIDER_ID\")\nSUBSCRIPTION_ID = os.getenv(\"SUBSCRIPTION_ID\")\njob_name = os.getenv(\"JOB_NAME\")\nproject_id = os.environ['PROJECT_ID']\n\nmlops_cos_credentials = {'API_KEY':API_KEY_MLOPS,\n                          'CRN':CRN_MLOPS,\n                          'AUTH_ENDPOINT':AUTH_ENDPOINT,\n                           'ENDPOINT_URL':ENDPOINT_URL_MLOPS,\n                           'BUCKET':BUCKET_MLOPS\n                          }\n\n\nauthenticator = IAMAuthenticator(apikey=CLOUD_API_KEY)\nwos_client = APIClient(authenticator=authenticator, service_instance_id=DATA_MART_ID)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## OpenScale Utils"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from operator import itemgetter\n\ndef get_monitor_instance_id_by_name(subscription_id,monitor):\n    for item in wos_client.monitor_instances.list().result.to_dict()['monitor_instances']:\n        if(item['entity']['monitor_definition_id']) ==monitor and item['entity']['target']['target_id']==subscription_id:\n            return item['metadata']['id']\n        \n        \ndef get_monitor_metrics(wos_client, monitor_instance_id):\n    start_time = datetime.now() - timedelta(days=7)\n    end_time = datetime.now()\n    runs = wos_client.monitor_instances.runs.list(\n        monitor_instance_id=monitor_instance_id\n    ).result.runs\n    measurements = ibm_watson_openscale.base_classes.watson_open_scale_v2.Measurements(\n        watson_open_scale=wos_client.monitor_instances._ai_client\n    )\n    for run in runs:\n        run_id = run.to_dict()[\"metadata\"][\"id\"]\n        response = measurements.list(\n            monitor_instance_id=monitor_instance_id,\n            start=start_time,\n            end=end_time,\n            run_id=run_id,\n        )\n        #print(response.result.to_dict())\n        metrics = response.result.to_dict()[\"measurements\"][0][\"entity\"][\"values\"][0][\n            \"metrics\"\n        ]\n        return metrics\n    \n    \ndef get_measurement_id(wos_client,monitor_instance_id):\n    start_time = datetime.now() - timedelta(days=7)\n    end_time = datetime.now()\n    runs = wos_client.monitor_instances.runs.list(monitor_instance_id=monitor_instance_id).result.runs\n    final =[]\n    for run in runs:\n        each = run.to_dict()[\"metadata\"][\"id\"]\n        response= wos_client.monitor_instances.measurements.list(monitor_instance_id=m_id,start=start_time,end=end_time,run_id=each)\n        result = response.result.to_dict()\n        final.append(result['measurements'][0]['metadata'])\n        #print(result['measurements'][0]['metadata'])\n    \n    my_asset_list = sorted(final, key=itemgetter(\"created_at\"), reverse=True)\n    return my_asset_list[0]['id']\n\n\ndef get_annotations(measurement_response):\n    sources_data = measurement_response.entity.sources[0].data\n    \n    annotations = {}\n    \n    if \"copy_of\" in sources_data:\n        original_measurement_id = sources_data[\"copy_of\"][\"measurement_id\"]\n    else:\n        original_measurement_id = measurement_response.metadata.id\n        \n    if \"drifted_transactions\" in sources_data:\n        annotations[\"drifted_transactions\"] = sorted([\"drift_{}_cluster_{}\".format(original_measurement_id, cluster[\"id\"]) for cluster in sources_data[\"drifted_transactions\"][\"clusters\"]])\n        \n    if \"data_drifted_transactions\" in sources_data:\n        annotations[\"data_drifted_transactions\"] = sorted([\"ddrift_{}_cluster_{}\".format(original_measurement_id, cluster[\"id\"]) for cluster in sources_data[\"data_drifted_transactions\"][\"clusters\"]])\n    \n    if \"model_data_drifted_transactions\" in sources_data:\n        annotations[\"model_data_drifted_transactions\"] = sorted([\"mddrift_{}_cluster_{}\".format(original_measurement_id, cluster[\"id\"]) for cluster in sources_data[\"drifted_transactions\"][\"clusters\"]])\n    \n    return annotations\n\ndef get_dataset_id(name,subscription_id):\n    import time\n    time.sleep(5)\n    data_set_id = None\n    if name == \"PAYLOAD_LOGGING\":\n        types = DataSetTypes.PAYLOAD_LOGGING\n    elif name == \"FEEDBACK\":\n        types = DataSetTypes.FEEDBACK \n    data_set_id = wos_client.data_sets.list(type=types, \n                                                target_target_id=subscription_id, \n                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\n    if data_set_id is None:\n        print(\"Payload data set not found. Please check subscription status.\")\n    \n    return data_set_id\n\ndef read_data_from_mlops_cos(key):\n    def __iter__(self): return 0\n    MLOPS_DATA_STORE_client = ibm_boto3.client(\n        service_name='s3',\n        ibm_api_key_id=API_KEY_MLOPS,\n        ibm_service_instance_id=CRN_MLOPS,\n        ibm_auth_endpoint=AUTH_ENDPOINT,\n        config=Config(signature_version='oauth'),\n        endpoint_url=ENDPOINT_URL_MLOPS)\n\n    body = MLOPS_DATA_STORE_client.get_object(Bucket='mlops-asset', Key=key)['Body']\n    # add missing __iter__ method, so pandas accepts body as file-like object\n    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n    gcf_df = pd.read_csv(body)\n    return gcf_df\ndef save_data_in_cos(df,filename,key,credentials,pipe=False):\n    \"\"\"\n\n    Save Data in IBM Cloud Object Storage\n\n    \"\"\"\n    try:\n        \n        if pipe:\n            with open (filename,'wb') as f:\n                pickle.dump(pipeline,f)\n        else:\n            df.to_csv(filename,index=False)\n        mlops_res = ibm_boto3.resource(\n            service_name='s3',\n            ibm_api_key_id=credentials['API_KEY'],\n            ibm_auth_endpoint=credentials['AUTH_ENDPOINT'],\n            config=Config(signature_version='oauth'),\n            endpoint_url=credentials['ENDPOINT_URL'])\n\n        mlops_res.Bucket(credentials['BUCKET']).upload_file(filename,key)\n        print(f\"File {filename} uploaded successfully\")\n    except Exception as e:\n        print(e)\n        print(\"File upload for {filename} failed\")\n        \n        \ndef push_driver(apikey, project_id, job_name):\n    jobrun = JobRunner(apikey, project_id)\n    job_id = jobrun.retrieve_job_id(name=job_name)\n    print(f\" Job ID is -->{job_id}\")\n\n    ##Start the Pipeline Run\n\n    jobrun.run_pipeline_job(job_id)\n\n    job_runs = jobrun.get_run_ids(job_id)\n\n    latest_job_run_id = list(job_runs.keys())[0]\n\n    print(f\"Latest Job RUN ID is -->{latest_job_run_id}\")\n\n    try:\n        while True:\n            result = jobrun.get_runs_by_runid(job_id=job_id, run_id=latest_job_run_id)\n            state = result[\"entity\"][\"job_run\"][\"state\"]\n            if state == \"Completed\":\n                break\n            elif state == \"Failed\":\n                raise Exception(\"Job Run Failed\")\n            print(state)\n            time.sleep(120)\n    except Exception as e:\n        print(e)\n\n    print(\n        f\"Job {job_id} with latest run {latest_job_run_id} done with status--> {state}\"\n    )\n    # pprint(jobrun.get_runs_by_runid(job_id=job_id, run_id=latest_job_id))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Get Measurement IDs and Extract Measurements"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "m_id = get_monitor_instance_id_by_name(SUBSCRIPTION_ID,'drift')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "measurement_id = get_measurement_id(wos_client,m_id)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "get_monitor_metrics(wos_client=wos_client,monitor_instance_id=m_id)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "measurement = wos_client.monitor_instances.measurements.get(measurement_id=measurement_id, monitor_instance_id=m_id).result"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "payload_data_set_id = get_dataset_id(name='PAYLOAD_LOGGING',subscription_id=SUBSCRIPTION_ID)\nfeedback_data_set_id = get_dataset_id(name='FEEDBACK',subscription_id=SUBSCRIPTION_ID)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Extract Drifted Records"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "drifted_df = wos_client.data_sets.get_list_of_records(data_set_id=payload_data_set_id, annotations=get_annotations(measurement)[\"data_drifted_transactions\"],output_type='pandas').result\ndrifted_df.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "discarded = ['asset_revision', 'debiased_probability','scoring_timestamp','scoring_id','probability','debiased_prediction','deployment_id']\ndrifted_df= drifted_df.drop(discarded,axis=1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "final_transactions = drifted_df[drifted_df['prediction_probability']>0.75]\nfinal_transactions= final_transactions.rename(columns={'prediction':'Risk'})\nfinal_transactions['Risk']= final_transactions['Risk'].map({1:'Risk',0:'No Risk'})\nfinal_transactions = final_transactions.drop('prediction_probability',axis=1)\nfinal_transactions['ForeignWorker'] = \"no\"\nfinal_transactions['InstallmentPlans'] = \"none\""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "final_transactions.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "original_training_data = read_data_from_mlops_cos(\"german_credit_risk_hist.csv\")\noriginal_training_data.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Combine the relevant high confidence records to the Training Data "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "new_df = pd.concat([final_transactions,original_training_data]).reset_index(drop=True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#save_data_in_cos(new_df,\"german_credit_risk.csv\",\"german_credit_risk.csv\",mlops_cos_credentials)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Kick Off the WS Pipeline. Supply the name of the job to run"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#push_driver(apikey=CLOUD_API_KEY,project_id=project_id,job_name=job_name)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}