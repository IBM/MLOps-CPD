{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true},"source":["## Training Notebook for a Custom PyTorch Lightning Model "]},{"cell_type":"markdown","metadata":{},"source":["Replace the train_models.ipynb notebook in your pipeline with this notebook to setup an MLOps workflow for a custom-built PyTorch Lightning model."]},{"cell_type":"markdown","metadata":{},"source":["### The following cell is a way to get the utility script required for this notebook. \n","Since IBM CPD SaaS doesn't have a filesystem, this is the only reliable way to get scripts on the cloud environment.\n","\n","```\n","!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git\n","```\n","\n","⚠️ Run the following cells only if you are executing on IBM CPD SaaS."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/iiias/MLOps-CPD.git"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!mv MLOps-CPD MLOps_CPD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from botocore.client import Config\n","from ibm_botocore.client import Config\n","from sklearn.model_selection import train_test_split,cross_val_score, KFold\n","from sklearn.metrics import roc_auc_score,confusion_matrix,plot_confusion_matrix,plot_roc_curve,f1_score,auc,roc_curve,accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n","from ibm_aigov_facts_client import AIGovFactsClient\n","from ibm_watson_studio_pipelines import WSPipelines\n","from ibm_watson_machine_learning import APIClient\n","import warnings\n","import os, types\n","import pandas as pd\n","import numpy as np\n","import ibm_boto3\n","import seaborn as sns\n","import json\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","from os.path import exists\n","if exists(\"utils/fs_utils.py\") and exists(\"utils/catalog_utils.py\"):\n","    from utils import fs_utils,catalog_utils\n","else:\n","    # If utils/fs_utils.py and utils/catalog_utils.py exist we assume that you are running on CPD SaaS\n","    # and will therefore import scripts from the freshly cloned repository\n","    from MLOps_CPD.utils import fs_utils, catalog_utils\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["## Succeeding cell contains the credentials for MLOps COS\n","```\n","## PROJECT COS \n","AUTH_ENDPOINT = \"https://iam.cloud.ibm.com/oidc/token\"\n","ENDPOINT_URL = \"https://s3.private.us.cloud-object-storage.appdomain.cloud\"\n","API_KEY_COS = \"xxx\"\n","BUCKET_PROJECT_COS = \"mlops-donotdelete-pr-qxxcecxi1dtw94\"\n","\n","##MLOPS COS\n","ENDPOINT_URL_MLOPS = \"https://s3.jp-tok.cloud-object-storage.appdomain.cloud\"\n","API_KEY_MLOPS = \"xxx\"\n","CRN_MLOPS = \"xxx\"\n","BUCKET_MLOPS  = \"mlops-asset\"\n","\n","##CATALOG\n","CATALOG_NAME = \"MLOps-ns\"\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Retrieve cos credentials from global pipeline parameters\n","import json\n","# Get json from environment and convert to string\n","project_cos_credentials = json.loads(os.getenv('project_cos_credentials'))\n","mlops_cos_credentials = json.loads(os.getenv('mlops_cos_credentials'))\n","\n","## PROJECT COS \n","AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n","ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n","API_KEY_COS = project_cos_credentials['API_KEY']\n","BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n","\n","## MLOPS COS\n","ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n","API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n","CRN_MLOPS = mlops_cos_credentials['CRN']\n","BUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"]},{"cell_type":"markdown","metadata":{},"source":["## Pipeline Params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# filename = os.getenv(\"feature_pickle\")\n","CLOUD_API_KEY = os.getenv(\"cloud_api_key\")\n","\n","MODEL_NAME =os.getenv(\"model_name\")\n","DEPLOYMENT_NAME =os.getenv(\"deployment_name\")\n","\n","project_id = os.environ['PROJECT_ID']\n","space_id = os.getenv(\"model_name\")\n","CATALOG_NAME = \"mlops-ns\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This variable will be the filename of the serialized model.\n","# DO NOT include a filetype in the variable name, as it will be serialized to ONNX and then archived as tar.gz with the same variable as name. \n","MODEL_SERIALIZED = \"gcr-torch_lightning\"\n","\n","MODEL_TYPE = \"pytorch-onnx_1.10\""]},{"cell_type":"markdown","metadata":{},"source":["## Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def download_file_cos(local_file_name,key):\n","    cos = ibm_boto3.client(service_name='s3',\n","                           ibm_api_key_id=API_KEY_MLOPS,\n","                           ibm_service_instance_id=CRN_MLOPS,\n","                           ibm_auth_endpoint=AUTH_ENDPOINT,\n","                           config=Config(signature_version='oauth'),\n","                           endpoint_url=ENDPOINT_URL_MLOPS)\n","    try:\n","        res=cos.download_file(Bucket=BUCKET_MLOPS,Key=key,Filename=local_file_name)\n","    except Exception as e:\n","        print(Exception, e)\n","    else:\n","        print('File Downloaded')\n","        \n","def read_data_from_mlops_cos(key):\n","    def __iter__(self): return 0\n","    MLOPS_DATA_STORE_client = ibm_boto3.client(\n","        service_name='s3',\n","        ibm_api_key_id=API_KEY_MLOPS,\n","        ibm_service_instance_id=CRN_MLOPS,\n","        ibm_auth_endpoint=AUTH_ENDPOINT,\n","        config=Config(signature_version='oauth'),\n","        endpoint_url=ENDPOINT_URL_MLOPS)\n","\n","    body = MLOPS_DATA_STORE_client.get_object(Bucket=BUCKET_MLOPS, Key=key)['Body']\n","    # add missing __iter__ method, so pandas accepts body as file-like object\n","    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n","\n","    gcf_df = pd.read_csv(body)\n","    return gcf_df\n","\n","def load_model(key, filename):\n","    download_file_cos(key,filename)\n","    with open (filename,\"rb\") as f:\n","        pipeline = pickle.load(f)\n","    return pipeline\n","\n","def check_if_file_exists(filename):\n","    mlops_client = ibm_boto3.client(\n","        service_name='s3',\n","        ibm_api_key_id=API_KEY_MLOPS,\n","        ibm_service_instance_id=CRN_MLOPS,\n","        ibm_auth_endpoint=AUTH_ENDPOINT,\n","        config=Config(signature_version='oauth'),\n","        endpoint_url=ENDPOINT_URL_MLOPS)\n","    \n","    for key in mlops_client.list_objects(Bucket=BUCKET_MLOPS)['Contents']:\n","        files = key['Key']\n","        if files == filename:\n","            return True\n","    return False\n","\n","def save_data_in_cos(dictionary,filename,key):\n","    \"\"\"\n","    Save Data in IBM Cloud Object Storage\n","    \"\"\"\n","    try:\n","        with open(filename+'.pkl', 'wb') as f:\n","            pickle.dump(dictionary, f)\n","        mlops_res = ibm_boto3.resource(\n","            service_name='s3',\n","            ibm_api_key_id=API_KEY_MLOPS,\n","            ibm_service_instance_id=CRN_MLOPS,\n","            ibm_auth_endpoint=AUTH_ENDPOINT,\n","            config=Config(signature_version='oauth'),\n","            endpoint_url=ENDPOINT_URL_MLOPS)\n","\n","        mlops_res.Bucket(BUCKET_MLOPS).upload_file(filename+'.pkl',key+'.pkl')\n","        print(f\"File {filename} uploaded successfully\")\n","    except Exception as e:\n","        print(e)\n","        print(\"File upload for {filename} failed\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The code was removed by Watson Studio for sharing."]},{"cell_type":"markdown","metadata":{},"source":["## Instantiate FactSheets Client"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["facts_client = AIGovFactsClient(api_key=CLOUD_API_KEY, experiment_name=\"CreditRiskModel\", container_type=\"project\", container_id=project_id, set_as_current_experiment=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Train Data and Test Data "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = read_data_from_mlops_cos('train_tfr.csv')\n","test_data = read_data_from_mlops_cos('test_tfr.csv')\n","train_data.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Load train and test set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_train = train_data['Risk']\n","\n","y_test = test_data['Risk']\n","\n","\n","X_train = train_data.drop('Risk',axis=1)\n","\n","X_test = test_data.drop('Risk',axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Make validation set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"]},{"cell_type":"markdown","metadata":{},"source":["## Define the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import pytorch_lightning as pl\n","import pandas as pd\n","\n","class CreditRiskClassifier(pl.LightningModule):\n","    def __init__(self, input_dim, hidden_dim=10, output_dim=1):\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x.squeeze()\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        x = x.float()\n","        y = y.float()\n","        y_hat = self(x)\n","        loss_fn = nn.BCEWithLogitsLoss()\n","        loss = loss_fn(y_hat, y)        \n","        self.log(f'train_loss_epoch_{self.trainer.current_epoch}', loss)\n","\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        x = x.float()\n","        y = y.float()\n","        y_hat = self(x)\n","        loss_fn = nn.BCEWithLogitsLoss()\n","        loss = loss_fn(y_hat, y)        \n","        self.log(f'val_loss_epoch_{self.trainer.current_epoch}', loss)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n","        return optimizer\n","\n","\n","# Dataset\n","class CreditRiskDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","        \n","    def __len__(self):\n","        return len(self.X)\n","        \n","    def __getitem__(self, idx):\n","        x = self.X[idx]\n","        x = torch.tensor(x, dtype=torch.float32)\n","        y = self.y[idx]\n","        y = torch.tensor(y, dtype=torch.float32)        \n","        return x, y\n"]},{"cell_type":"markdown","metadata":{},"source":["## Instantiate and train the Model "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split into train and val sets\n","X_train = pd.get_dummies(X_tr).values\n","y_train = y_tr.to_numpy()\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val1 = pd.get_dummies(X_val).values\n","X_val1 = scaler.fit_transform(X_val1)\n","y_val = y_val.to_numpy()\n","\n","input_dim = X_train.shape[1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_epochs = 100\n","\n","# Create datasets and dataloaders\n","train_dataset = CreditRiskDataset(X_train, y_train)\n","val_dataset = CreditRiskDataset(X_val1, y_val)\n","\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=128)\n","\n","# Initialize model and trainer\n","model = CreditRiskClassifier(input_dim=input_dim)\n","trainer = pl.Trainer(max_epochs=n_epochs, progress_bar_refresh_rate=20)\n","\n","# Train the model and collect losses\n","t = trainer.fit(model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Collect historic loss (loss per epoch)\n","train_loss = [trainer.callback_metrics[f'train_loss_epoch_{epoch_idx}'] for epoch_idx in range(n_epochs)]\n","val_loss = [trainer.callback_metrics[f'val_loss_epoch_{epoch_idx}'] for epoch_idx in range(n_epochs)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot historic loss curves\n","plt.plot(range(n_epochs), train_loss, label=\"train_loss\")\n","plt.plot(range(n_epochs), val_loss, label=\"val_loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Save train and val loss to COS"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["save_data_in_cos(val_loss,'val_loss','val_loss')\n","save_data_in_cos(train_loss,'train_loss','train_loss')"]},{"cell_type":"markdown","metadata":{},"source":["## Check if the files are copied in COS\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files_copied_in_cos = check_if_file_exists('val_loss.pkl') and check_if_file_exists('val_loss.pkl')\n","files_copied_in_cos"]},{"cell_type":"markdown","metadata":{},"source":["## Baseline Results of the Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get predictions for test data\n","inputs = torch.from_numpy(X_val1).float()\n","outputs = model(inputs)\n","pred = torch.round(torch.sigmoid(outputs))\n","pred = pd.DataFrame(pred.detach().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate score for area under ROC\n","print(roc_auc_score(y_val,pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_cm = confusion_matrix(y_val,pred)\n","\n","group_names = ['True Neg','False Pos','False Neg','True Pos']\n","group_counts = [\"{0:0.0f}\".format(value) for value in df_cm.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in df_cm.flatten()/np.sum(df_cm)]\n","labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","sns.heatmap(df_cm, annot=labels, fmt='', cmap='Blues')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fpr, tpr, thresholds = roc_curve(y_val, pred)\n","\n","fig, ax = plt.subplots()\n","ax.plot(fpr, tpr)\n","ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.0])\n","plt.rcParams['font.size'] = 12\n","\n","plt.xlabel('False Positive Rate (1 - Specificity)')\n","plt.ylabel('True Positive Rate (Sensitivity)')\n","plt.grid(True)\n","\n","print(\"\\n\")\n","print (\"Area Under Curve: %.2f\" %auc(fpr, tpr))\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Serialize Model to ONNX and compress"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save model\n","torch.save(model.state_dict(), f'{MODEL_SERIALIZED}.pt')\n","\n","# Load model\n","loaded_model = CreditRiskClassifier(X_train.shape[1])\n","loaded_model.load_state_dict(torch.load(f'{MODEL_SERIALIZED}.pt'))\n","\n","#loaded_model.eval()\n","\n","# Convert to ONNX format\n","dummy_input = torch.randn(1, X_train.shape[1])\n","torch.onnx.export(\n","    loaded_model, \n","    dummy_input,\n","    f'{MODEL_SERIALIZED}.onnx',\n","    export_params=True, \n","    verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tarfile\n","\n","def onnx_to_tar(onnx_path, tar_file_path):\n","    with tarfile.open(tar_file_path, \"w:gz\") as tar:\n","        tar.add(onnx_path, arcname=os.path.basename(onnx_path))\n","\n","onnx_path = f\"{MODEL_SERIALIZED}.onnx\"\n","tar_file_path = f\"{MODEL_SERIALIZED}.tar.gz\"\n","onnx_to_tar(onnx_path, tar_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["WML_CREDENTIALS = {\n","                   \"url\": \"https://us-south.ml.cloud.ibm.com\",\n","                   \"apikey\": CLOUD_API_KEY\n","            }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wml_client = APIClient(WML_CREDENTIALS)"]},{"cell_type":"markdown","metadata":{},"source":["## Save and Log Models in AI Factsheets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_train = train_data['Risk']\n","X_train = train_data.drop('Risk',axis=1)\n","\n","def save_log_facts():\n","    ctutils = catalog_utils.CatalogUtils(\n","        service_url=\"https://api.dataplatform.cloud.ibm.com\",\n","        api_key=CLOUD_API_KEY,\n","        project_id=project_id,\n","        auth_url=\"https://iam.cloud.ibm.com/identity/token\")\n","    catalog_id = ctutils.get_catalog_id_map()[CATALOG_NAME]\n","    fsutils = fs_utils.FSUtils(wml_client=wml_client,catalog_id=catalog_id,project_id=project_id,bss_account_id='27ff418fedd6aedffb8dc6ae4164a1d2',space_id=space_id,facts_client=facts_client)\n","    train_ref  = fsutils.prepare_training_reference(apikey=CLOUD_API_KEY,crn=CRN_MLOPS,bucket_name=BUCKET_MLOPS,endpoint=ENDPOINT_URL_MLOPS,training_file_name=\"german_credit_risk.csv\")\n","    model_id = fsutils.save_model(model=tar_file_path,model_name=MODEL_NAME,model_entry_name=\"MLOps\",model_entry_description=\"MLOps Model Entry\",target=\"Risk\",X=X_train,y=y_train,train_data_ref=train_ref, model_type=\"pytorch-onnx_1.10\")\n","    \n","    nb_name = \"train_models\"\n","    nb_asset_id = \"b8d38cab-e373-4303-bd09-12e1086c9132\"\n","    CPD_URL =\"https://dataplatform.cloud.ibm.com\"\n","\n","    nb_asset_url = \"https://\" + CPD_URL + \"/analytics/notebooks/v2/\" + nb_asset_id + \"?projectid=\" + project_id + \"&context=cpdaas\"\n","\n","    latestRunId = facts_client.runs.list_runs_by_experiment('1').sort_values('start_time').iloc[-1]['run_id']\n","    facts_client.runs.set_tags(latestRunId, {\"Notebook name\": nb_name, \"Notebook id\": nb_asset_id, \"Notebook URL\" : nb_asset_url})\n","    facts_client.export_facts.export_payload(latestRunId)\n","\n","    RUN_ID=facts_client.runs.get_current_run_id()\n","    facts_client.export_facts.export_payload(RUN_ID)\n","    \n","    return model_id\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_id = save_log_facts()"]},{"cell_type":"markdown","metadata":{},"source":["## Save Params in WS Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_params = {}\n","train_params['auc_roc'] = float(auc(fpr, tpr))\n","train_params['training_done'] = True\n","train_params['model_name'] = MODEL_NAME\n","train_params['deployment_name'] = DEPLOYMENT_NAME\n","train_params['model_id'] = model_id\n","train_params['project_id'] = project_id\n","# train_params['model_pipeline'] = \"/home/wsuser/work/model_pipeline.pkl\"\n","\n","pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n","pipelines_client.store_results(train_params)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"}}},"nbformat":4,"nbformat_minor":1}
