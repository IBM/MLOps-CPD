{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deserialize Artifact and Load Into Memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Mission\n",
    "\n",
    "This is a **very basic** continuous validation test - which acts more as a playground for you to do miscellaneous testing - that you may append to an CI testing pipeline. \n",
    "Within this test model or pipeline binary (which has been uploaded to COS in the train models notebook [train_models.ipynb](https://github.com/IBM/MLOps-CPD/blob/master/train_models.ipynb)) is downloaded. \n",
    "We then try to deserialize it and confirm it is found properly in-memory by invoking its predict method on the head of the test data.\n",
    "\n",
    "When using this asset to construct an MLOps pipeline for a more complex Machine Learning task, we suggest you test your model/pipeline more rigorously. \n",
    "To scratch the surface on what this could look like, you may test more methods of your model/pipeline object or maybe even set a footprint threshold for the binary's filesize or the memory footprint after deserialization.\n",
    "\n",
    "Happy testing! ðŸ¥³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from ibm_watson_studio_pipelines import WSPipelines\n",
    "from botocore.client import Config\n",
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import credentials  # for local development"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Get Cloud API Key from Global Pipeline Parameters\n",
    "CLOUD_API_KEY = os.getenv('cloud_api_key')\n",
    "\n",
    "# At this point, the filename of the binary is hardcoded. \n",
    "# With increasing complexity of your MLOps flow, you may want to add the filename as a pipeline parameter.\n",
    "FILENAME = \"model_pipeline.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Retrieve cos credentials from pipeline parameters\n",
    "import json\n",
    "\n",
    "# Get json from environment and convert to string\n",
    "project_cos_credentials = json.loads(os.environ['project_cos_credentials'])\n",
    "mlops_cos_credentials = json.loads(os.getenv('mlops_cos_credentials'))\n",
    "\n",
    "## PROJECT COS \n",
    "AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n",
    "ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n",
    "API_KEY_COS = project_cos_credentials['API_KEY']\n",
    "#BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n",
    "\n",
    "## MLOPS COS\n",
    "ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n",
    "API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n",
    "CRN_MLOPS = mlops_cos_credentials['CRN']\n",
    "BUCKET_MLOPS = mlops_cos_credentials['BUCKET']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download model pipeline and deserialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Reusable methods for COS operations and reading\n",
    "def read_data_from_mlops_cos(key):\n",
    "    def __iter__(self): return 0\n",
    "\n",
    "    MLOPS_DATA_STORE_client = ibm_boto3.client(\n",
    "        service_name='s3',\n",
    "        ibm_api_key_id=API_KEY_MLOPS,\n",
    "        ibm_service_instance_id=CRN_MLOPS,\n",
    "        ibm_auth_endpoint=AUTH_ENDPOINT,\n",
    "        config=Config(signature_version='oauth'),\n",
    "        endpoint_url=ENDPOINT_URL_MLOPS)\n",
    "\n",
    "    body = MLOPS_DATA_STORE_client.get_object(Bucket=BUCKET_MLOPS, Key=key)['Body']\n",
    "    # add missing __iter__ method, so pandas accepts body as file-like object\n",
    "    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType(__iter__, body)\n",
    "\n",
    "    gcf_df = pd.read_csv(body)\n",
    "    return gcf_df\n",
    "\n",
    "\n",
    "def download_file_cos(local_file_name, key):\n",
    "    cos = ibm_boto3.client(service_name='s3',\n",
    "                             ibm_api_key_id=API_KEY_MLOPS,\n",
    "                             ibm_service_instance_id=CRN_MLOPS,\n",
    "                             ibm_auth_endpoint=AUTH_ENDPOINT,\n",
    "                             config=Config(signature_version='oauth'),\n",
    "                             endpoint_url=ENDPOINT_URL_MLOPS)\n",
    "    try:\n",
    "        res = cos.download_file(Bucket=BUCKET_MLOPS, Key=key, Filename=local_file_name)\n",
    "    except Exception as e:\n",
    "        print(Exception, e)\n",
    "    else:\n",
    "        print('File Downloaded')\n",
    "\n",
    "\n",
    "def load_model(key, filename):\n",
    "    download_file_cos(key, filename)\n",
    "    with open(filename, \"rb\") as f:\n",
    "        print(sys.getsizeof(f))\n",
    "        artifact = pickle.load(f)\n",
    "    return artifact\n",
    "\n",
    "\n",
    "def get_file_size_cos(local_file_name, key):\n",
    "    cos = ibm_boto3.resource(service_name='s3',\n",
    "                             ibm_api_key_id=API_KEY_MLOPS,\n",
    "                             ibm_service_instance_id=CRN_MLOPS,\n",
    "                             ibm_auth_endpoint=AUTH_ENDPOINT,\n",
    "                             config=Config(signature_version='oauth'),\n",
    "                             endpoint_url=ENDPOINT_URL_MLOPS)\n",
    "    try:\n",
    "       # size = cos.ObjectSummary(local_file_name, key).size ### this doesnt work for some reason\n",
    "        size = cos.Bucket(BUCKET_MLOPS).Object(key).content_length\n",
    "    except Exception as e:\n",
    "        print(Exception, e)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download test_data from cos and take first five rows\n",
    "test_data = read_data_from_mlops_cos('test_tfr.csv').head(5)\n",
    "\n",
    "# Drop only 'Risk'\n",
    "X_test = test_data.drop(list(test_data.columns)[-1:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Downloaded\n",
      "4264\n"
     ]
    }
   ],
   "source": [
    "# Import the corresponding module/classifier for the module to ensure successful deserialization and subsequent usability\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Download pickled model from Cloud Object Storage and deserialize\n",
    "model_pipeline = load_model(FILENAME, FILENAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "memory_footprint = get_file_size_cos(FILENAME, FILENAME)\n",
    "binary_cos_footprint = sys.getsizeof(model_pipeline)\n",
    "binary_deserializable = False\n",
    "\n",
    "try:\n",
    "    # Try to predict the output variable for five rows of test data\n",
    "    model_pipeline.predict(X_test)\n",
    "    binary_deserializable = True\n",
    "except Exception as e:\n",
    "    # Could make prediction. There might be a fault in model (de)serialization or in the model itself.\n",
    "    print(\"Could not score. Possibly because model could not be loaded into memory?\")\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save your test results to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['deserializable'] = binary_deserializable\n",
    "params['memory_footprint'] = memory_footprint\n",
    "params['binary_cos_footprint'] = binary_cos_footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n",
    "pipelines_client.store_results(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}