{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deserialize Artifact and Load Into Memory"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Mission\n",
                "\n",
                "This is a **very basic** continuous validation test - which acts more as a playground for you to do miscellaneous testing - that you may append to an CI testing pipeline. \n",
                "Within this test model or pipeline binary which is uploaded to COS in the train models notebook ([train_models.ipynb](https://github.com/IBM/MLOps-CPD/blob/master/train_models.ipynb)) is downloaded. \n",
                "We then try to deserialize it and confirm it is found properly memory by invoking its predict method on the head of the test data.\n",
                "\n",
                "When using this asset to construct an MLOps pipeline for a more complex Machine Learning task, we suggest you test your model/pipeline more rigorously. \n",
                "To scratch the surface on what this could look like, you may test more methods of your model/pipeline object or maybe even set a footprint threshold for the binary's filesize or the memory footprint after deserialization.\n",
                "\n",
                "Happy testing! ðŸ¥³"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pickle\n",
                "from ibm_watson_studio_pipelines import WSPipelines\n",
                "from botocore.client import Config\n",
                "from ibm_botocore.client import Config\n",
                "import ibm_boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Get Cloud API Key from Global Pipeline Parameters\n",
                "CLOUD_API_KEY = os.getenv('CLOUD_API_KEY')\n",
                "\n",
                "# At this point, the filename of the binary is hardcoded. \n",
                "# With increasing complexity of your MLOps flow, you may want to add the filename as a pipeline parameter.\n",
                "FILENAME = \"model_pipeline.pkl\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Retrieve cos credentials from pipeline parameters\n",
                "import json\n",
                "# Get json from environment and convert to string\n",
                "project_cos_credentials = json.loads(os.environ['project_cos_credentials'])\n",
                "mlops_cos_credentials = json.loads(os.getenv('mlops_cos_credentials'))\n",
                "\n",
                "## PROJECT COS \n",
                "AUTH_ENDPOINT = project_cos_credentials['AUTH_ENDPOINT']\n",
                "ENDPOINT_URL = project_cos_credentials['ENDPOINT_URL']\n",
                "API_KEY_COS = project_cos_credentials['API_KEY']\n",
                "BUCKET_PROJECT_COS = project_cos_credentials['BUCKET']\n",
                "\n",
                "## MLOPS COS\n",
                "ENDPOINT_URL_MLOPS = mlops_cos_credentials['ENDPOINT_URL']\n",
                "API_KEY_MLOPS = mlops_cos_credentials['API_KEY']\n",
                "CRN_MLOPS = mlops_cos_credentials['CRN']\n",
                "BUCKET_MLOPS  = mlops_cos_credentials['BUCKET']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Download model pipeline and deserialize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reusable methods for COS operations and reading\n",
                "def read_data_from_mlops_cos(key):\n",
                "    def __iter__(self): return 0\n",
                "    MLOPS_DATA_STORE_client = ibm_boto3.client(\n",
                "        service_name='s3',\n",
                "        ibm_api_key_id=API_KEY_MLOPS,\n",
                "        ibm_service_instance_id=CRN_MLOPS,\n",
                "        ibm_auth_endpoint=AUTH_ENDPOINT,\n",
                "        config=Config(signature_version='oauth'),\n",
                "        endpoint_url=ENDPOINT_URL_MLOPS)\n",
                "\n",
                "    body = MLOPS_DATA_STORE_client.get_object(Bucket=BUCKET_MLOPS, Key=key)['Body']\n",
                "    # add missing __iter__ method, so pandas accepts body as file-like object\n",
                "    if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
                "\n",
                "    gcf_df = pd.read_csv(body)\n",
                "    return gcf_df\n",
                "\n",
                "\n",
                "def download_file_cos(local_file_name,key):\n",
                "    cos = ibm_boto3.client(service_name='s3',\n",
                "                           ibm_api_key_id=API_KEY_MLOPS,\n",
                "                           ibm_service_instance_id=CRN_MLOPS,\n",
                "                           ibm_auth_endpoint=AUTH_ENDPOINT,\n",
                "                           config=Config(signature_version='oauth'),\n",
                "                           endpoint_url=ENDPOINT_URL_MLOPS)\n",
                "    try:\n",
                "        res=cos.download_file(Bucket=BUCKET_MLOPS,Key=key,Filename=local_file_name)\n",
                "    except Exception as e:\n",
                "        print(Exception, e)\n",
                "    else:\n",
                "        print('File Downloaded')\n",
                "\n",
                "def load_model(key, filename):\n",
                "    download_file_cos(key, filename)\n",
                "    with open (filename, \"rb\") as f:\n",
                "        artifact = pickle.load(f)\n",
                "    return artifact"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the corresponding module/classifier for the module\n",
                "from lightgbm import LGBMClassifier\n",
                "\n",
                "# Download pickled model from Cloud Object Storage and deserialize\n",
                "model_pipeline = load_model(FILENAME, FILENAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Download testdata from cos and take first five rows\n",
                "test_data = read_data_from_mlops_cos('test_tfr.csv').head(5)\n",
                "\n",
                "# Drop only 'Risk'\n",
                "X_test = test_data.drop(list(test_data.columns)[-1:],axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "binary_deserializable = False\n",
                "\n",
                "try:\n",
                "    # Try to predict the output variable for five rows of test data\n",
                "    model_pipeline.predict(X_test)\n",
                "    binary_deserializable = True\n",
                "except Exception as e:\n",
                "    # Could make prediction. There might be a fault in model (de)serialization or in the model itself.\n",
                "    print(\"Could not score. Possibly because model could not be loaded into memory\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save your test results to pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {}\n",
                "params['deserializable'] = binary_deserializable\n",
                "# params['memory_footprint'] = ...\n",
                "# params['binary_footprint'] = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n",
                "pipelines_client.store_results(params)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "vscode": {
            "interpreter": {
                "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
